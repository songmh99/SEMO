{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d28e011-3bdf-4697-b2bb-cfb62ce5dc2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Baran: The Error Correction System\n",
    "# Mohammad Mahdavi\n",
    "# moh.mahdavi.l@gmail.com\n",
    "# April 2019\n",
    "# Big Data Management Group\n",
    "# TU Berlin\n",
    "# All Rights Reserved\n",
    "########################################\n",
    "\n",
    "\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import io\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import pickle\n",
    "import difflib\n",
    "import unicodedata\n",
    "import multiprocessing\n",
    "\n",
    "import bs4\n",
    "import bz2\n",
    "import numpy\n",
    "import py7zr\n",
    "import mwparserfromhell\n",
    "import sklearn.svm\n",
    "import sklearn.ensemble\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.linear_model\n",
    "\n",
    "import raha\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from detection import Detection\n",
    "import signal\n",
    "from datetime import datetime\n",
    "\n",
    "def check_string(string: str):\n",
    "    if re.search(r\"-inner_error-\", string):\n",
    "        return \"-inner_error-\" + string[-6:-4]\n",
    "    elif re.search(r\"-outer_error-\", string):\n",
    "        return \"-outer_error-\" + string[-6:-4]\n",
    "    elif re.search(r\"-inner_outer_error-\", string):\n",
    "        return \"-inner_outer_error-\" + string[-6:-4]\n",
    "    elif re.search(r\"-dirty-original_error-\", string):\n",
    "        return \"-original_error-\" + string[-9:-4]\n",
    "\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Time exceeded\")\n",
    "########################################\n",
    "\n",
    "\n",
    "########################################\n",
    "class Correction:\n",
    "    \"\"\"\n",
    "    The main class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        The constructor.\n",
    "        \"\"\"\n",
    "        self.PRETRAINED_VALUE_BASED_MODELS_PATH = \"\"\n",
    "        self.VALUE_ENCODINGS = [\"identity\", \"unicode\"]\n",
    "        self.CLASSIFICATION_MODEL = \"ABC\"   # [\"ABC\", \"DTC\", \"GBC\", \"GNB\", \"KNC\" ,\"SGDC\", \"SVC\"]\n",
    "        self.IGNORE_SIGN = \"<<<IGNORE_THIS_VALUE>>>\"\n",
    "        self.VERBOSE = False\n",
    "        self.SAVE_RESULTS = False\n",
    "        self.ONLINE_PHASE = False\n",
    "        self.LABELING_BUDGET = 20\n",
    "        self.MIN_CORRECTION_CANDIDATE_PROBABILITY = 0.0\n",
    "        self.MIN_CORRECTION_OCCURRENCE = 2\n",
    "        self.MAX_VALUE_LENGTH = 50\n",
    "        self.REVISION_WINDOW_SIZE = 5\n",
    "\n",
    "    @staticmethod\n",
    "    def _wikitext_segmenter(wikitext):\n",
    "        \"\"\"\n",
    "        This method takes a Wikipedia page revision text in wikitext and segments it recursively.\n",
    "        \"\"\"\n",
    "        def recursive_segmenter(node):\n",
    "            if isinstance(node, str):\n",
    "                segments_list.append(node)\n",
    "            elif isinstance(node, mwparserfromhell.nodes.text.Text):\n",
    "                segments_list.append(node.value)\n",
    "            elif not node:\n",
    "                pass\n",
    "            elif isinstance(node, mwparserfromhell.wikicode.Wikicode):\n",
    "                for n in node.nodes:\n",
    "                    if isinstance(n, str):\n",
    "                        recursive_segmenter(n)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.text.Text):\n",
    "                        recursive_segmenter(n.value)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.heading.Heading):\n",
    "                        recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.tag.Tag):\n",
    "                        recursive_segmenter(n.contents)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.wikilink.Wikilink):\n",
    "                        if n.text:\n",
    "                            recursive_segmenter(n.text)\n",
    "                        else:\n",
    "                            recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.external_link.ExternalLink):\n",
    "                        # recursive_parser(n.url)\n",
    "                        recursive_segmenter(n.title)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.template.Template):\n",
    "                        recursive_segmenter(n.name)\n",
    "                        for p in n.params:\n",
    "                            # recursive_parser(p.name)\n",
    "                            recursive_segmenter(p.value)\n",
    "                    elif isinstance(n, mwparserfromhell.nodes.html_entity.HTMLEntity):\n",
    "                        segments_list.append(n.normalize())\n",
    "                    elif not n or isinstance(n, mwparserfromhell.nodes.comment.Comment) or \\\n",
    "                            isinstance(n, mwparserfromhell.nodes.argument.Argument):\n",
    "                        pass\n",
    "                    else:\n",
    "                        sys.stderr.write(\"Inner layer unknown node found: {}, {}\\n\".format(type(n), n))\n",
    "            else:\n",
    "                sys.stderr.write(\"Outer layer unknown node found: {}, {}\\n\".format(type(node), node))\n",
    "\n",
    "        try:\n",
    "            parsed_wikitext = mwparserfromhell.parse(wikitext)\n",
    "        except:\n",
    "            parsed_wikitext = \"\"\n",
    "        segments_list = []\n",
    "        recursive_segmenter(parsed_wikitext)\n",
    "        return segments_list\n",
    "\n",
    "    def extract_revisions(self, wikipedia_dumps_folder):\n",
    "        \"\"\"\n",
    "        This method takes the folder path of Wikipedia page revision history dumps and extracts the value-based corrections.\n",
    "        \"\"\"\n",
    "        rd_folder_path = os.path.join(wikipedia_dumps_folder, \"revision-data\")\n",
    "        if not os.path.exists(rd_folder_path):\n",
    "            os.mkdir(rd_folder_path)\n",
    "        compressed_dumps_list = [df for df in os.listdir(wikipedia_dumps_folder) if df.endswith(\".7z\")]\n",
    "        page_counter = 0\n",
    "        for file_name in compressed_dumps_list:\n",
    "            compressed_dump_file_path = os.path.join(wikipedia_dumps_folder, file_name)\n",
    "            dump_file_name, _ = os.path.splitext(os.path.basename(compressed_dump_file_path))\n",
    "            rdd_folder_path = os.path.join(rd_folder_path, dump_file_name)\n",
    "            if not os.path.exists(rdd_folder_path):\n",
    "                os.mkdir(rdd_folder_path)\n",
    "            else:\n",
    "                continue\n",
    "            archive = py7zr.SevenZipFile(compressed_dump_file_path, mode=\"r\")\n",
    "            archive.extractall(path=wikipedia_dumps_folder)\n",
    "            archive.close()\n",
    "            decompressed_dump_file_path = os.path.join(wikipedia_dumps_folder, dump_file_name)\n",
    "            decompressed_dump_file = io.open(decompressed_dump_file_path, \"r\", encoding=\"utf-8\")\n",
    "            page_text = \"\"\n",
    "            for i, line in enumerate(decompressed_dump_file):\n",
    "                line = line.strip()\n",
    "                if line == \"<page>\":\n",
    "                    page_text = \"\"\n",
    "                page_text += \"\\n\" + line\n",
    "                if line == \"</page>\":\n",
    "                    revisions_list = []\n",
    "                    page_tree = bs4.BeautifulSoup(page_text, \"html.parser\")\n",
    "                    previous_text = \"\"\n",
    "                    for revision_tag in page_tree.find_all(\"revision\"):\n",
    "                        revision_text = revision_tag.find(\"text\").text\n",
    "                        if previous_text:\n",
    "                            a = [t for t in self._wikitext_segmenter(previous_text) if t]\n",
    "                            b = [t for t in self._wikitext_segmenter(revision_text) if t]\n",
    "                            s = difflib.SequenceMatcher(None, a, b)\n",
    "                            for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "                                if tag == \"equal\":\n",
    "                                    continue\n",
    "                                revisions_list.append({\n",
    "                                    \"old_value\": a[i1:i2],\n",
    "                                    \"new_value\": b[j1:j2],\n",
    "                                    \"left_context\": a[i1 - self.REVISION_WINDOW_SIZE:i1],\n",
    "                                    \"right_context\": a[i2:i2 + self.REVISION_WINDOW_SIZE]\n",
    "                                })\n",
    "                        previous_text = revision_text\n",
    "                    if revisions_list:\n",
    "                        page_counter += 1\n",
    "                        if self.VERBOSE and page_counter % 100 == 0:\n",
    "                            for entry in revisions_list:\n",
    "                                print(\"----------Page Counter:---------\\n\", page_counter,\n",
    "                                      \"\\n----------Old Value:---------\\n\", entry[\"old_value\"],\n",
    "                                      \"\\n----------New Value:---------\\n\", entry[\"new_value\"],\n",
    "                                      \"\\n----------Left Context:---------\\n\", entry[\"left_context\"],\n",
    "                                      \"\\n----------Right Context:---------\\n\", entry[\"right_context\"],\n",
    "                                      \"\\n==============================\")\n",
    "                        json.dump(revisions_list, open(os.path.join(rdd_folder_path, page_tree.id.text + \".json\"), \"w\"))\n",
    "            decompressed_dump_file.close()\n",
    "            os.remove(decompressed_dump_file_path)\n",
    "            if self.VERBOSE:\n",
    "                print(\"{} ({} / {}) is processed.\".format(file_name, len(os.listdir(rd_folder_path)), len(compressed_dumps_list)))\n",
    "\n",
    "    @staticmethod\n",
    "    def _value_encoder(value, encoding):\n",
    "        \"\"\"\n",
    "        This method represents a value with a specified value abstraction encoding method.\n",
    "        \"\"\"\n",
    "        if encoding == \"identity\":\n",
    "            return json.dumps(list(value))\n",
    "        if encoding == \"unicode\":\n",
    "            return json.dumps([unicodedata.category(c) for c in value])\n",
    "\n",
    "    @staticmethod\n",
    "    def _to_model_adder(model, key, value):\n",
    "        \"\"\"\n",
    "        This methods incrementally adds a key-value into a dictionary-implemented model.\n",
    "        \"\"\"\n",
    "        if key not in model:\n",
    "            model[key] = {}\n",
    "        if value not in model[key]:\n",
    "            model[key][value] = 0.0\n",
    "        model[key][value] += 1.0\n",
    "\n",
    "    def _value_based_models_updater(self, models, ud):\n",
    "        \"\"\"\n",
    "        This method updates the value-based error corrector models with a given update dictionary.\n",
    "        \"\"\"\n",
    "        # TODO: adding jabeja konannde bakhshahye substring\n",
    "        if self.ONLINE_PHASE or (ud[\"new_value\"] and len(ud[\"new_value\"]) <= self.MAX_VALUE_LENGTH and\n",
    "                                 ud[\"old_value\"] and len(ud[\"old_value\"]) <= self.MAX_VALUE_LENGTH and\n",
    "                                 ud[\"old_value\"] != ud[\"new_value\"] and ud[\"old_value\"].lower() != \"n/a\" and\n",
    "                                 not ud[\"old_value\"][0].isdigit()):\n",
    "            remover_transformation = {}\n",
    "            adder_transformation = {}\n",
    "            replacer_transformation = {}\n",
    "            s = difflib.SequenceMatcher(None, ud[\"old_value\"], ud[\"new_value\"])\n",
    "            for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "                index_range = json.dumps([i1, i2])\n",
    "                if tag == \"delete\":\n",
    "                    remover_transformation[index_range] = \"\"\n",
    "                if tag == \"insert\":\n",
    "                    adder_transformation[index_range] = ud[\"new_value\"][j1:j2]\n",
    "                if tag == \"replace\":\n",
    "                    replacer_transformation[index_range] = ud[\"new_value\"][j1:j2]\n",
    "            for encoding in self.VALUE_ENCODINGS:\n",
    "                encoded_old_value = self._value_encoder(ud[\"old_value\"], encoding)\n",
    "                if remover_transformation:\n",
    "                    self._to_model_adder(models[0], encoded_old_value, json.dumps(remover_transformation))\n",
    "                if adder_transformation:\n",
    "                    self._to_model_adder(models[1], encoded_old_value, json.dumps(adder_transformation))\n",
    "                if replacer_transformation:\n",
    "                    self._to_model_adder(models[2], encoded_old_value, json.dumps(replacer_transformation))\n",
    "                self._to_model_adder(models[3], encoded_old_value, ud[\"new_value\"])\n",
    "\n",
    "    def pretrain_value_based_models(self, revision_data_folder):\n",
    "        \"\"\"\n",
    "        This method pretrains value-based error corrector models.\n",
    "        \"\"\"\n",
    "        def _models_pruner():\n",
    "            for mi, model in enumerate(models):\n",
    "                for k in list(model.keys()):\n",
    "                    for v in list(model[k].keys()):\n",
    "                        if model[k][v] < self.MIN_CORRECTION_OCCURRENCE:\n",
    "                            models[mi][k].pop(v)\n",
    "                    if not models[mi][k]:\n",
    "                        models[mi].pop(k)\n",
    "\n",
    "        models = [{}, {}, {}, {}]\n",
    "        rd_folder_path = revision_data_folder\n",
    "        page_counter = 0\n",
    "        for folder in os.listdir(rd_folder_path):\n",
    "            if os.path.isdir(os.path.join(rd_folder_path, folder)):\n",
    "                for rf in os.listdir(os.path.join(rd_folder_path, folder)):\n",
    "                    if rf.endswith(\".json\"):\n",
    "                        page_counter += 1\n",
    "                        if page_counter % 100000 == 0:\n",
    "                            _models_pruner()\n",
    "                            if self.VERBOSE:\n",
    "                                print(page_counter, \"pages are processed.\")\n",
    "                        try:\n",
    "                            revision_list = json.load(io.open(os.path.join(rd_folder_path, folder, rf), encoding=\"utf-8\"))\n",
    "                        except:\n",
    "                            continue\n",
    "                        for rd in revision_list:\n",
    "                            update_dictionary = {\n",
    "                                \"old_value\": raha.dataset.Dataset.value_normalizer(\"\".join(rd[\"old_value\"])),\n",
    "                                \"new_value\": raha.dataset.Dataset.value_normalizer(\"\".join(rd[\"new_value\"]))\n",
    "                            }\n",
    "                            self._value_based_models_updater(models, update_dictionary)\n",
    "        _models_pruner()\n",
    "        pretrained_models_path = os.path.join(revision_data_folder, \"pretrained_value_based_models.dictionary\")\n",
    "        if self.PRETRAINED_VALUE_BASED_MODELS_PATH:\n",
    "            pretrained_models_path = self.PRETRAINED_VALUE_BASED_MODELS_PATH\n",
    "        pickle.dump(models, bz2.BZ2File(pretrained_models_path, \"wb\"))\n",
    "        if self.VERBOSE:\n",
    "            print(\"The pretrained value-based models are stored in {}.\".format(pretrained_models_path))\n",
    "\n",
    "    def _vicinity_based_models_updater(self, models, ud):\n",
    "        \"\"\"\n",
    "        This method updates the vicinity-based error corrector models with a given update dictionary.\n",
    "        \"\"\"\n",
    "        for j, cv in enumerate(ud[\"vicinity\"]):\n",
    "            if cv != self.IGNORE_SIGN:\n",
    "                self._to_model_adder(models[j][ud[\"column\"]], cv, ud[\"new_value\"])\n",
    "\n",
    "    def _domain_based_model_updater(self, model, ud):\n",
    "        \"\"\"\n",
    "        This method updates the domain-based error corrector model with a given update dictionary.\n",
    "        \"\"\"\n",
    "        self._to_model_adder(model, ud[\"column\"], ud[\"new_value\"])\n",
    "\n",
    "    def _value_based_corrector(self, models, ed):\n",
    "        \"\"\"\n",
    "        This method takes the value-based models and an error dictionary to generate potential value-based corrections.\n",
    "        \"\"\"\n",
    "        results_list = []\n",
    "        for m, model_name in enumerate([\"remover\", \"adder\", \"replacer\", \"swapper\"]):\n",
    "            model = models[m]\n",
    "            for encoding in self.VALUE_ENCODINGS:\n",
    "                results_dictionary = {}\n",
    "                encoded_value_string = self._value_encoder(ed[\"old_value\"], encoding)\n",
    "                if encoded_value_string in model:\n",
    "                    sum_scores = sum(model[encoded_value_string].values())\n",
    "                    if model_name in [\"remover\", \"adder\", \"replacer\"]:\n",
    "                        for transformation_string in model[encoded_value_string]:\n",
    "                            index_character_dictionary = {i: c for i, c in enumerate(ed[\"old_value\"])}\n",
    "                            transformation = json.loads(transformation_string)\n",
    "                            for change_range_string in transformation:\n",
    "                                change_range = json.loads(change_range_string)\n",
    "                                if model_name in [\"remover\", \"replacer\"]:\n",
    "                                    for i in range(change_range[0], change_range[1]):\n",
    "                                        index_character_dictionary[i] = \"\"\n",
    "                                if model_name in [\"adder\", \"replacer\"]:\n",
    "                                    ov = \"\" if change_range[0] not in index_character_dictionary else \\\n",
    "                                        index_character_dictionary[change_range[0]]\n",
    "                                    index_character_dictionary[change_range[0]] = transformation[change_range_string] + ov\n",
    "                            new_value = \"\"\n",
    "                            for i in range(len(index_character_dictionary)):\n",
    "                                new_value += index_character_dictionary[i]\n",
    "                            pr = model[encoded_value_string][transformation_string] / sum_scores\n",
    "                            if pr >= self.MIN_CORRECTION_CANDIDATE_PROBABILITY:\n",
    "                                results_dictionary[new_value] = pr\n",
    "                    if model_name == \"swapper\":\n",
    "                        for new_value in model[encoded_value_string]:\n",
    "                            pr = model[encoded_value_string][new_value] / sum_scores\n",
    "                            if pr >= self.MIN_CORRECTION_CANDIDATE_PROBABILITY:\n",
    "                                results_dictionary[new_value] = pr\n",
    "                results_list.append(results_dictionary)\n",
    "        return results_list\n",
    "\n",
    "    def _vicinity_based_corrector(self, models, ed):\n",
    "        \"\"\"\n",
    "        This method takes the vicinity-based models and an error dictionary to generate potential vicinity-based corrections.\n",
    "        \"\"\"\n",
    "        results_list = []\n",
    "        for j, cv in enumerate(ed[\"vicinity\"]):\n",
    "            results_dictionary = {}\n",
    "            if j != ed[\"column\"] and cv in models[j][ed[\"column\"]]:\n",
    "                sum_scores = sum(models[j][ed[\"column\"]][cv].values())\n",
    "                for new_value in models[j][ed[\"column\"]][cv]:\n",
    "                    pr = models[j][ed[\"column\"]][cv][new_value] / sum_scores\n",
    "                    if pr >= self.MIN_CORRECTION_CANDIDATE_PROBABILITY:\n",
    "                        results_dictionary[new_value] = pr\n",
    "            results_list.append(results_dictionary)\n",
    "        return results_list\n",
    "\n",
    "    def _domain_based_corrector(self, model, ed):\n",
    "        \"\"\"\n",
    "        This method takes a domain-based model and an error dictionary to generate potential domain-based corrections.\n",
    "        \"\"\"\n",
    "        results_dictionary = {}\n",
    "        sum_scores = sum(model[ed[\"column\"]].values())\n",
    "        for new_value in model[ed[\"column\"]]:\n",
    "            pr = model[ed[\"column\"]][new_value] / sum_scores\n",
    "            if pr >= self.MIN_CORRECTION_CANDIDATE_PROBABILITY:\n",
    "                results_dictionary[new_value] = pr\n",
    "        return [results_dictionary]\n",
    "\n",
    "    def initialize_dataset(self, d):\n",
    "        \"\"\"\n",
    "        This method initializes the dataset.\n",
    "        \"\"\"\n",
    "        self.ONLINE_PHASE = True\n",
    "        d.results_folder = os.path.join(os.path.dirname(d.path), \"raha-baran-results-\" + d.name)\n",
    "        if self.SAVE_RESULTS and not os.path.exists(d.results_folder):\n",
    "            os.mkdir(d.results_folder)\n",
    "        d.column_errors = {}\n",
    "        for cell in d.detected_cells:\n",
    "            self._to_model_adder(d.column_errors, cell[1], cell)\n",
    "        d.labeled_tuples = {} if not hasattr(d, \"labeled_tuples\") else d.labeled_tuples\n",
    "        d.labeled_cells = {} if not hasattr(d, \"labeled_cells\") else d.labeled_cells\n",
    "        d.corrected_cells = {} if not hasattr(d, \"corrected_cells\") else d.corrected_cells\n",
    "        return d\n",
    "\n",
    "    def initialize_models(self, d):\n",
    "        \"\"\"\n",
    "        This method initializes the error corrector models.\n",
    "        \"\"\"\n",
    "        d.value_models = [{}, {}, {}, {}]\n",
    "        if os.path.exists(self.PRETRAINED_VALUE_BASED_MODELS_PATH):\n",
    "            d.value_models = pickle.load(bz2.BZ2File(self.PRETRAINED_VALUE_BASED_MODELS_PATH, \"rb\"))\n",
    "            if self.VERBOSE:\n",
    "                print(\"The pretrained value-based models are loaded.\")\n",
    "        d.vicinity_models = {j: {jj: {} for jj in range(d.dataframe.shape[1])} for j in range(d.dataframe.shape[1])}\n",
    "        d.domain_models = {}\n",
    "        for row in d.dataframe.itertuples():\n",
    "            i, row = row[0], row[1:]\n",
    "            vicinity_list = [cv if (i, cj) not in d.detected_cells else self.IGNORE_SIGN for cj, cv in enumerate(row)]\n",
    "            for j, value in enumerate(row):\n",
    "                if (i, j) not in d.detected_cells:\n",
    "                    temp_vicinity_list = list(vicinity_list)\n",
    "                    temp_vicinity_list[j] = self.IGNORE_SIGN\n",
    "                    update_dictionary = {\n",
    "                        \"column\": j,\n",
    "                        \"new_value\": value,\n",
    "                        \"vicinity\": temp_vicinity_list\n",
    "                    }\n",
    "                    self._vicinity_based_models_updater(d.vicinity_models, update_dictionary)\n",
    "                    self._domain_based_model_updater(d.domain_models, update_dictionary)\n",
    "        if self.VERBOSE:\n",
    "            print(\"The error corrector models are initialized.\")\n",
    "\n",
    "    def sample_tuple(self, d):\n",
    "        \"\"\"\n",
    "        This method samples a tuple.\n",
    "        \"\"\"\n",
    "        remaining_column_erroneous_cells = {}\n",
    "        remaining_column_erroneous_values = {}\n",
    "        for j in d.column_errors:\n",
    "            for cell in d.column_errors[j]:\n",
    "                if cell not in d.corrected_cells:\n",
    "                    self._to_model_adder(remaining_column_erroneous_cells, j, cell)\n",
    "                    self._to_model_adder(remaining_column_erroneous_values, j, d.dataframe.iloc[cell])\n",
    "        tuple_score = numpy.ones(d.dataframe.shape[0])\n",
    "        tuple_score[list(d.labeled_tuples.keys())] = 0.0\n",
    "        for j in remaining_column_erroneous_cells:\n",
    "            for cell in remaining_column_erroneous_cells[j]:\n",
    "                value = d.dataframe.iloc[cell]\n",
    "                column_score = math.exp(len(remaining_column_erroneous_cells[j]) / len(d.column_errors[j]))\n",
    "                cell_score = math.exp(remaining_column_erroneous_values[j][value] / len(remaining_column_erroneous_cells[j]))\n",
    "                tuple_score[cell[0]] *= column_score * cell_score\n",
    "        d.sampled_tuple = numpy.random.choice(numpy.argwhere(tuple_score == numpy.amax(tuple_score)).flatten())\n",
    "        if self.VERBOSE:\n",
    "            print(\"Tuple {} is sampled.\".format(d.sampled_tuple))\n",
    "\n",
    "    def label_with_ground_truth(self, d):\n",
    "        \"\"\"\n",
    "        This method labels a tuple with ground truth.\n",
    "        \"\"\"\n",
    "        d.labeled_tuples[d.sampled_tuple] = 1\n",
    "        for j in range(d.dataframe.shape[1]):\n",
    "            cell = (d.sampled_tuple, j)\n",
    "            error_label = 0\n",
    "            if d.dataframe.iloc[cell] != d.clean_dataframe.iloc[cell]:\n",
    "                error_label = 1\n",
    "            d.labeled_cells[cell] = [error_label, d.clean_dataframe.iloc[cell]]\n",
    "        if self.VERBOSE:\n",
    "            print(\"Tuple {} is labeled.\".format(d.sampled_tuple))\n",
    "\n",
    "    def update_models(self, d):\n",
    "        \"\"\"\n",
    "        This method updates the error corrector models with a new labeled tuple.\n",
    "        \"\"\"\n",
    "        cleaned_sampled_tuple = [d.labeled_cells[(d.sampled_tuple, j)][1] for j in range(d.dataframe.shape[1])]\n",
    "        for j in range(d.dataframe.shape[1]):\n",
    "            cell = (d.sampled_tuple, j)\n",
    "            update_dictionary = {\n",
    "                \"column\": cell[1],\n",
    "                \"old_value\": d.dataframe.iloc[cell],\n",
    "                \"new_value\": cleaned_sampled_tuple[j],\n",
    "            }\n",
    "            if d.labeled_cells[cell][0] == 1:\n",
    "                if cell not in d.detected_cells:\n",
    "                    d.detected_cells[cell] = self.IGNORE_SIGN\n",
    "                    self._to_model_adder(d.column_errors, cell[1], cell)\n",
    "                self._value_based_models_updater(d.value_models, update_dictionary)\n",
    "                self._domain_based_model_updater(d.domain_models, update_dictionary)\n",
    "                update_dictionary[\"vicinity\"] = [cv if j != cj else self.IGNORE_SIGN\n",
    "                                                 for cj, cv in enumerate(cleaned_sampled_tuple)]\n",
    "            else:\n",
    "                update_dictionary[\"vicinity\"] = [cv if j != cj and d.labeled_cells[(d.sampled_tuple, cj)][0] == 1\n",
    "                                                 else self.IGNORE_SIGN for cj, cv in enumerate(cleaned_sampled_tuple)]\n",
    "            self._vicinity_based_models_updater(d.vicinity_models, update_dictionary)\n",
    "        if self.VERBOSE:\n",
    "            print(\"The error corrector models are updated with new labeled tuple {}.\".format(d.sampled_tuple))\n",
    "\n",
    "    def _feature_generator_process(self, args):\n",
    "        \"\"\"\n",
    "        This method generates features for each data column in a parallel process.\n",
    "        \"\"\"\n",
    "        d, cell = args\n",
    "        error_dictionary = {\"column\": cell[1], \"old_value\": d.dataframe.iloc[cell], \"vicinity\": list(d.dataframe.iloc[cell[0], :])}\n",
    "        value_corrections = self._value_based_corrector(d.value_models, error_dictionary)\n",
    "        vicinity_corrections = self._vicinity_based_corrector(d.vicinity_models, error_dictionary)\n",
    "        domain_corrections = self._domain_based_corrector(d.domain_models, error_dictionary)\n",
    "        models_corrections = value_corrections + vicinity_corrections + domain_corrections\n",
    "        corrections_features = {}\n",
    "        for mi, model in enumerate(models_corrections):\n",
    "            for correction in model:\n",
    "                if correction not in corrections_features:\n",
    "                    corrections_features[correction] = numpy.zeros(len(models_corrections))\n",
    "                corrections_features[correction][mi] = model[correction]\n",
    "        return corrections_features\n",
    "\n",
    "    def generate_features(self, d):\n",
    "        \"\"\"\n",
    "        This method generates a feature vector for each pair of a data error and a potential correction.\n",
    "        \"\"\"\n",
    "        d.pair_features = {}\n",
    "        pairs_counter = 0\n",
    "        process_args_list = [[d, cell] for cell in d.detected_cells]\n",
    "        pool = multiprocessing.Pool()\n",
    "        feature_generation_results = pool.map(self._feature_generator_process, process_args_list)\n",
    "        pool.close()\n",
    "        for ci, corrections_features in enumerate(feature_generation_results):\n",
    "            cell = process_args_list[ci][1]\n",
    "            d.pair_features[cell] = {}\n",
    "            for correction in corrections_features:\n",
    "                d.pair_features[cell][correction] = corrections_features[correction]\n",
    "                pairs_counter += 1\n",
    "        if self.VERBOSE:\n",
    "            print(\"{} pairs of (a data error, a potential correction) are featurized.\".format(pairs_counter))\n",
    "\n",
    "    def predict_corrections(self, d):\n",
    "        \"\"\"\n",
    "        This method predicts\n",
    "        \"\"\"\n",
    "        \n",
    "        for j in d.column_errors:\n",
    "            x_train = []\n",
    "            y_train = []\n",
    "            x_test = []\n",
    "            test_cell_correction_list = []\n",
    "            for k, cell in enumerate(d.column_errors[j]):\n",
    "                if cell in d.pair_features:\n",
    "                    candidates_set = []\n",
    "                    actual_errors = d.clean_dataframe\n",
    "                    for correction in d.pair_features[cell]:\n",
    "                        candidates_set.append(correction)\n",
    "                        if cell in d.labeled_cells and d.labeled_cells[cell][0] == 1:\n",
    "                            x_train.append(d.pair_features[cell][correction])\n",
    "                            y_train.append(int(correction == d.labeled_cells[cell][1]))\n",
    "                            d.corrected_cells[cell] = d.labeled_cells[cell][1]\n",
    "                        else:\n",
    "                            x_test.append(d.pair_features[cell][correction])\n",
    "                            test_cell_correction_list.append([cell, correction])\n",
    "                    if d.clean_dataframe.iloc[cell[0], cell[1]] in candidates_set:\n",
    "                        clean_in_cands.append(cell)\n",
    "            if self.CLASSIFICATION_MODEL == \"ABC\":\n",
    "                classification_model = sklearn.ensemble.AdaBoostClassifier(n_estimators=100)\n",
    "            if self.CLASSIFICATION_MODEL == \"DTC\":\n",
    "                classification_model = sklearn.tree.DecisionTreeClassifier(criterion=\"gini\")\n",
    "            if self.CLASSIFICATION_MODEL == \"GBC\":\n",
    "                classification_model = sklearn.ensemble.GradientBoostingClassifier(n_estimators=100)\n",
    "            if self.CLASSIFICATION_MODEL == \"GNB\":\n",
    "                classification_model = sklearn.naive_bayes.GaussianNB()\n",
    "            if self.CLASSIFICATION_MODEL == \"KNC\":\n",
    "                classification_model = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "            if self.CLASSIFICATION_MODEL == \"SGDC\":\n",
    "                classification_model = sklearn.linear_model.SGDClassifier(loss=\"hinge\", penalty=\"l2\")\n",
    "            if self.CLASSIFICATION_MODEL == \"SVC\":\n",
    "                classification_model = sklearn.svm.SVC(kernel=\"sigmoid\")\n",
    "            if x_train and x_test:\n",
    "                if sum(y_train) == 0:\n",
    "                    predicted_labels = numpy.zeros(len(x_test))\n",
    "                elif sum(y_train) == len(y_train):\n",
    "                    predicted_labels = numpy.ones(len(x_test))\n",
    "                else:\n",
    "                    classification_model.fit(x_train, y_train)\n",
    "                    predicted_labels = classification_model.predict(x_test)\n",
    "                # predicted_probabilities = classification_model.predict_proba(x_test)\n",
    "                # correction_confidence = {}\n",
    "                for index, predicted_label in enumerate(predicted_labels):\n",
    "                    cell, predicted_correction = test_cell_correction_list[index]\n",
    "                    # confidence = predicted_probabilities[index][1]\n",
    "                    if predicted_label:\n",
    "                        # if cell not in correction_confidence or confidence > correction_confidence[cell]:\n",
    "                        #     correction_confidence[cell] = confidence\n",
    "                        d.corrected_cells[cell] = predicted_correction\n",
    "        if self.VERBOSE:\n",
    "            print(\"{:.0f}% ({} / {}) of data errors are corrected.\".format(100 * len(d.corrected_cells) / len(d.detected_cells),\n",
    "                                                                           len(d.corrected_cells), len(d.detected_cells)))\n",
    "\n",
    "    def store_results(self, d):\n",
    "        \"\"\"\n",
    "        This method stores the results.\n",
    "        \"\"\"\n",
    "        ec_folder_path = os.path.join(d.results_folder, \"error-correction\")\n",
    "        if not os.path.exists(ec_folder_path):\n",
    "            os.mkdir(ec_folder_path)\n",
    "        pickle.dump(d, open(os.path.join(ec_folder_path, \"correction.dataset\"), \"wb\"))\n",
    "        if self.VERBOSE:\n",
    "            print(\"The results are stored in {}.\".format(os.path.join(ec_folder_path, \"correction.dataset\")))\n",
    "\n",
    "    def run(self, d):\n",
    "        \"\"\"\n",
    "        This method runs Baran on an input dataset to correct data errors.\n",
    "        \"\"\"\n",
    "        if self.VERBOSE:\n",
    "            print(\"------------------------------------------------------------------------\\n\"\n",
    "                  \"---------------------Initialize the Dataset Object----------------------\\n\"\n",
    "                  \"------------------------------------------------------------------------\")\n",
    "        d = self.initialize_dataset(d)\n",
    "        if self.VERBOSE:\n",
    "            print(\"------------------------------------------------------------------------\\n\"\n",
    "                  \"--------------------Initialize Error Corrector Models-------------------\\n\"\n",
    "                  \"------------------------------------------------------------------------\")\n",
    "        self.initialize_models(d)\n",
    "        if self.VERBOSE:\n",
    "            print(\"------------------------------------------------------------------------\\n\"\n",
    "                  \"--------------Iterative Tuple Sampling, Labeling, and Learning----------\\n\"\n",
    "                  \"------------------------------------------------------------------------\")\n",
    "        # while len(d.labeled_tuples) < self.LABELING_BUDGET:\n",
    "        #     self.sample_tuple(d)\n",
    "        #     if d.has_ground_truth:\n",
    "        #         self.label_with_ground_truth(d)\n",
    "            # else:\n",
    "            #   In this case, user should label the tuple interactively as shown in the Jupyter notebook.\n",
    "        for si in d.labeled_tuples:\n",
    "            d.sampled_tuple = si\n",
    "            self.update_models(d)\n",
    "            self.generate_features(d)\n",
    "            self.predict_corrections(d)\n",
    "            if self.VERBOSE:\n",
    "                print(\"------------------------------------------------------------------------\")\n",
    "        if self.SAVE_RESULTS:\n",
    "            if self.VERBOSE:\n",
    "                print(\"------------------------------------------------------------------------\\n\"\n",
    "                      \"---------------------------Storing the Results--------------------------\\n\"\n",
    "                      \"------------------------------------------------------------------------\")\n",
    "            # self.store_results(d)\n",
    "        return d.corrected_cells\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a7a52a-4766-400a-9245-6e6aa66973cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0255\n",
      "0.11208791208791209\n",
      "<_io.TextIOWrapper name='../Exp_result/raha_baran/hospital_bclean/onlyED_hospital_bclean2-inner_outer_error-09.txt' mode='w' encoding='UTF-8'>\n",
      "16.687933206558228\n",
      "0.014\n",
      "0.06153846153846154\n",
      "<_io.TextIOWrapper name='../Exp_result/raha_baran/hospital_bclean/oriED+EC_hospital_bclean2-inner_outer_error-09.txt' mode='w' encoding='UTF-8'>\n",
      "64.65689587593079\n",
      "rep_right:28\n",
      "rec_right:28\n",
      "wrong_cells:455\n",
      "prec:0.014\n",
      "rec:0.06153846153846154\n",
      "wrong2right:28\n",
      "right2right:0\n",
      "wrong2wrong:23\n",
      "right2wrong:1949\n",
      "clean_in_cands BEFORE filter:883\n",
      "clean_in_cands AFTER filter:883\n",
      "proportion of clean value in candidates:0.44149999999779255\n",
      "proportion of clean value in candidates and selected correctly:0.03171007927483907\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument('--clean_path', type=str, default=None)\n",
    "#     parser.add_argument('--dirty_path', type=str, default=None)\n",
    "#     parser.add_argument('--task_name', type=str, default=None)\n",
    "#     args = parser.parse_args()\n",
    "#     dirty_path = args.dirty_path\n",
    "#     clean_path = args.clean_path\n",
    "#     task_name = args.task_name\n",
    "\n",
    "# task_name = \"tax1\"\n",
    "# clean_path = \"../data_with_rules/tax/split_data/tax-dirty-original_error-1.csv\"\n",
    "# dirty_path = \"../data_with_rules/tax/split_data/tax-dirty-original_error-1.csv\"\n",
    "\n",
    "clean_path = \"../data_with_rules/hospital/hospital_clean_bclean9.csv\"\n",
    "dirty_path = \"../data_with_rules/hospital/hospital-inner_outer_error-09.csv\"\n",
    "task_name = \"hospital_bclean2\"\n",
    "\n",
    "# dirty_path = \"../../BClean/dataset/flights/flights-inner_outer_error-02.csv\"\n",
    "# clean_path = \"../../BClean/dataset/flights/flights_clean.csv\"\n",
    "# task_name = \"flights_bclean2\"\n",
    "\n",
    "# clean_path = 'D:/WorkSpace/Code/2024/Automatic-Data-Repair-main/data_with_rules/tax/split_data/tax-clean-clean_data_ori-0010k.csv'\n",
    "# dirty_path = \"./data_with_rules/tax/split_data/tax-dirty-original_error-0010k.csv\"\n",
    "\n",
    "stra_path = \"../data_with_rules/\" + task_name[:-1] + \"/noise/raha-baran-results-\" + task_name + check_string(dirty_path)\n",
    "\n",
    "if os.path.exists(stra_path):\n",
    "    shutil.rmtree(stra_path)\n",
    "stra_path = \"../DATASET/data_with_rules/\" + task_name[:-1] + \"/noise/raha-baran-results-\" + task_name + check_string(dirty_path)\n",
    "if os.path.exists(stra_path):\n",
    "    shutil.rmtree(stra_path)\n",
    "stra_path = \"../data_with_rules/tax/split_data/raha-baran-results-\" + task_name + check_string(dirty_path)\n",
    "if os.path.exists(stra_path):\n",
    "    shutil.rmtree(stra_path)\n",
    "stra_path = \"../data_with_rules/tax/split_data/raha-baran-results-\" + task_name + check_string(dirty_path)\n",
    "if os.path.exists(stra_path):\n",
    "    shutil.rmtree(stra_path)\n",
    "dataset_name = task_name\n",
    "\n",
    "dataset_dictionary = {\n",
    "    \"name\": task_name + check_string(dirty_path),\n",
    "    \"path\": dirty_path,\n",
    "    \"clean_path\": clean_path\n",
    "}\n",
    "time_limit = 24*3600\n",
    "# signal.signal(signal.SIGALRM, handler)\n",
    "# signal.alarm(time_limit)\n",
    "clean_in_cands = []\n",
    "try:\n",
    "    # get Raha result\n",
    "    time_start = time.time()\n",
    "    app1 = Detection()\n",
    "    detected_cells = app1.run(dataset_dictionary)\n",
    "    p, r, f = app1.d.get_data_cleaning_evaluation(detected_cells)[:3]\n",
    "    time_end = time.time()\n",
    "\n",
    "    out_path = \"../Exp_result/raha_baran/\" + task_name[:-1] + \"/onlyED_\" + task_name + check_string(dirty_path) + \".txt\"\n",
    "    if not os.path.exists(os.path.dirname(out_path)):\n",
    "        os.makedirs(os.path.dirname(out_path))\n",
    "    \n",
    "    f = open(out_path, 'w')\n",
    "    # sys.stdout = f\n",
    "    print(\"{}\\n{}\\n{}\".format(p, r, f))\n",
    "    print(time_end-time_start)\n",
    "    f.close()\n",
    "\n",
    "    time_start = time.time()\n",
    "    data = raha.dataset.Dataset(dataset_dictionary)\n",
    "    app = Correction()\n",
    "    correction_dictionary = app.run(app1.d)\n",
    "    p, r, f = data.get_data_cleaning_evaluation(correction_dictionary)[-3:]\n",
    "\n",
    "    out_path = \"../Exp_result/raha_baran/\" + task_name[:-1] + \"/oriED+EC_\" + task_name + check_string(dirty_path) + \".txt\"\n",
    "    res_path = \"../Repaired_res/raha_baran/\" + task_name[:-1] + \"/repaired_\" + task_name + check_string(dirty_path) + \".csv\"\n",
    "    if not os.path.exists(os.path.dirname(res_path)):\n",
    "        os.makedirs(os.path.dirname(res_path))\n",
    "    repaired_df = pd.read_csv(dirty_path)\n",
    "    for cell, value in correction_dictionary.items():\n",
    "        repaired_df.iloc[cell[0], cell[1]] = value\n",
    "    repaired_df.to_csv(res_path, index=False, columns=list(repaired_df.columns))\n",
    "\n",
    "    f = open(out_path, 'w')\n",
    "    # sys.stdout = f\n",
    "    print(\"{}\\n{}\\n{}\".format(p, r, str(f)))\n",
    "    time_end = time.time()\n",
    "    print(time_end-time_start)\n",
    "    f.close()\n",
    "    # --------------------\n",
    "    # app.extract_revisions(wikipedia_dumps_folder=\"../wikipedia-data\")\n",
    "    # app.pretrain_value_based_models(revision_data_folder=\"../wikipedia-data/revision-data\")\n",
    "\n",
    "    # sys.stdout = sys.__stdout__ \n",
    "    out_path = \"../Exp_result/raha_baran/\" + task_name[:-1] + \"/all_compute_\" + task_name + check_string(dirty_path) + \".txt\"\n",
    "    f = open(out_path, 'w')\n",
    "    # sys.stdout = f\n",
    "    actual_errors = data.get_actual_errors_dictionary()\n",
    "    actual_errors_list = list(actual_errors.keys())\n",
    "    repaired_cells = list(correction_dictionary.keys())\n",
    "    right2wrong = 0\n",
    "    right2right = 0\n",
    "    wrong2right = 0\n",
    "    wrong2wrong = 0\n",
    "    rep_right = 0\n",
    "    rec_right = 0\n",
    "\n",
    "    rep_total = len(repaired_cells)\n",
    "    wrong_cells = len(actual_errors_list)\n",
    "    repair_right_cells = []\n",
    "    for cell in repaired_cells:\n",
    "        try:\n",
    "            if correction_dictionary[cell] == actual_errors[cell]:\n",
    "                repair_right_cells.append(cell)\n",
    "                rep_right += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for cell in actual_errors_list:\n",
    "        try:\n",
    "            if cell in repaired_cells:\n",
    "                if correction_dictionary[cell] == actual_errors[cell]:\n",
    "                    rec_right += 1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    for cell in repair_right_cells:\n",
    "        if cell in actual_errors_list:\n",
    "            wrong2right += 1\n",
    "        else:\n",
    "            right2right += 1\n",
    "\n",
    "    print(\"rep_right:\"+str(rep_right))\n",
    "    print(\"rec_right:\"+str(rec_right))\n",
    "    print(\"wrong_cells:\"+str(wrong_cells))\n",
    "    print(\"prec:\"+str(p))\n",
    "    print(\"rec:\"+str(r))\n",
    "    print(\"wrong2right:\"+str(wrong2right))\n",
    "    print(\"right2right:\"+str(right2right))\n",
    "    repair_wrong_cells = [i for i in repaired_cells if i not in repair_right_cells]\n",
    "    for cell in repair_wrong_cells:\n",
    "        if cell in actual_errors_list:\n",
    "            wrong2wrong += 1\n",
    "        else:\n",
    "            right2wrong += 1\n",
    "    print(\"wrong2wrong:\"+str(wrong2wrong))\n",
    "    print(\"right2wrong:\"+str(right2wrong))\n",
    "    print(\"clean_in_cands BEFORE filter:\"+str(len(clean_in_cands)))\n",
    "    clean_in_cands = list(set(clean_in_cands))\n",
    "    for cell in clean_in_cands:\n",
    "        if cell not in repaired_cells:\n",
    "            clean_in_cands.remove(cell)\n",
    "    print(\"clean_in_cands AFTER filter:\"+str(len(clean_in_cands)))\n",
    "    print(\"proportion of clean value in candidates:\"+str(len(clean_in_cands)/(rep_total+1e-8)))\n",
    "    clean_in_cands_repair_right = []\n",
    "    for cell in clean_in_cands:\n",
    "        if cell in repair_right_cells:\n",
    "            clean_in_cands_repair_right.append(cell)\n",
    "    print(\"proportion of clean value in candidates and selected correctly:\"+str(len(clean_in_cands_repair_right)/(len(clean_in_cands)+1e-8)))\n",
    "    f.close()\n",
    "########################################\n",
    "except TimeoutError as e: \n",
    "    print(\"Time exceeded:\", e, task_name, dirty_path)\n",
    "    out_file = open(\"../aggre_results/timeout_log.txt\", \"a\")\n",
    "    now = datetime.now()\n",
    "    out_file.write(now.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    out_file.write(\"Baran with Raha.py: \")\n",
    "    out_file.write(f\" {task_name}\")\n",
    "    out_file.write(f\" {dirty_path}\\n\")\n",
    "    out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a2fb84-32a6-423e-ae15-1dcc9ec595de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6549e59-31de-4408-ab72-3bbaa1391e94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(921, 1), (903, 1), (922, 1), (921, 12), (24, 12), (43, 12), (51, 12), (157, 12), (318, 12), (327, 12), (332, 12), (350, 12), (374, 12), (389, 12), (410, 12), (492, 12), (521, 12), (536, 12), (564, 12), (579, 12), (587, 12), (628, 12), (674, 12), (797, 12), (809, 12), (866, 12), (889, 12), (919, 12)]\n"
     ]
    }
   ],
   "source": [
    "w2r = []\n",
    "for cell in repair_right_cells:\n",
    "    if cell in actual_errors_list:\n",
    "        wrong2right += 1\n",
    "        w2r.append(cell)\n",
    "    else:\n",
    "        right2right += 1\n",
    "print(w2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adbb480b-4e7c-4db0-9b3f-457b8c70c79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b40de47-bb81-40cb-a241-edce29b4213e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baran = [(921, 'HospitalName'), (903, 'HospitalName'), (922, 'HospitalName'), (921, 'EmergencyService'), (24, 'EmergencyService'), (43, 'EmergencyService'), (51, 'EmergencyService'), (157, 'EmergencyService'), (318, 'EmergencyService'), (327, 'EmergencyService'), (332, 'EmergencyService'), (350, 'EmergencyService'), (374, 'EmergencyService'), (389, 'EmergencyService'), (410, 'EmergencyService'), (492, 'EmergencyService'), (521, 'EmergencyService'), (536, 'EmergencyService'), (564, 'EmergencyService'), (579, 'EmergencyService'), (587, 'EmergencyService'), (628, 'EmergencyService'), (674, 'EmergencyService'), (797, 'EmergencyService'), (809, 'EmergencyService'), (866, 'EmergencyService'), (889, 'EmergencyService'), (919, 'EmergencyService')]\n"
     ]
    }
   ],
   "source": [
    "a = [(i[0], repaired_df.columns[i[1]]) for i in w2r]\n",
    "print(f\"baran = {a}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bead9-f888-4664-9dd4-6e8d4957b084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90267e3-9703-4c17-b518-4cc47c917d06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prx",
   "language": "python",
   "name": "prx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
